[
    {
        "label": "annotations",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "site",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "site",
        "description": "site",
        "detail": "site",
        "documentation": {}
    },
    {
        "label": "sys",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sys",
        "description": "sys",
        "detail": "sys",
        "documentation": {}
    },
    {
        "label": "OllamaEmbeddings",
        "importPath": "langchain_community.embeddings.ollama",
        "description": "langchain_community.embeddings.ollama",
        "isExtraImport": true,
        "detail": "langchain_community.embeddings.ollama",
        "documentation": {}
    },
    {
        "label": "OllamaEmbeddings",
        "importPath": "langchain_community.embeddings.ollama",
        "description": "langchain_community.embeddings.ollama",
        "isExtraImport": true,
        "detail": "langchain_community.embeddings.ollama",
        "documentation": {}
    },
    {
        "label": "OllamaEmbeddings",
        "importPath": "langchain_community.embeddings.ollama",
        "description": "langchain_community.embeddings.ollama",
        "isExtraImport": true,
        "detail": "langchain_community.embeddings.ollama",
        "documentation": {}
    },
    {
        "label": "OllamaEmbeddings",
        "importPath": "langchain_community.embeddings.ollama",
        "description": "langchain_community.embeddings.ollama",
        "isExtraImport": true,
        "detail": "langchain_community.embeddings.ollama",
        "documentation": {}
    },
    {
        "label": "Chroma",
        "importPath": "langchain_community.vectorstores.chroma",
        "description": "langchain_community.vectorstores.chroma",
        "isExtraImport": true,
        "detail": "langchain_community.vectorstores.chroma",
        "documentation": {}
    },
    {
        "label": "Chroma",
        "importPath": "langchain_community.vectorstores.chroma",
        "description": "langchain_community.vectorstores.chroma",
        "isExtraImport": true,
        "detail": "langchain_community.vectorstores.chroma",
        "documentation": {}
    },
    {
        "label": "Chroma",
        "importPath": "langchain_community.vectorstores.chroma",
        "description": "langchain_community.vectorstores.chroma",
        "isExtraImport": true,
        "detail": "langchain_community.vectorstores.chroma",
        "documentation": {}
    },
    {
        "label": "Chroma",
        "importPath": "langchain_community.vectorstores.chroma",
        "description": "langchain_community.vectorstores.chroma",
        "isExtraImport": true,
        "detail": "langchain_community.vectorstores.chroma",
        "documentation": {}
    },
    {
        "label": "ChatOllama",
        "importPath": "langchain_community.chat_models.ollama",
        "description": "langchain_community.chat_models.ollama",
        "isExtraImport": true,
        "detail": "langchain_community.chat_models.ollama",
        "documentation": {}
    },
    {
        "label": "ChatOllama",
        "importPath": "langchain_community.chat_models.ollama",
        "description": "langchain_community.chat_models.ollama",
        "isExtraImport": true,
        "detail": "langchain_community.chat_models.ollama",
        "documentation": {}
    },
    {
        "label": "google.generativeai",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "google.generativeai",
        "description": "google.generativeai",
        "detail": "google.generativeai",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "ChatGoogleGenerativeAI",
        "importPath": "langchain_google_genai",
        "description": "langchain_google_genai",
        "isExtraImport": true,
        "detail": "langchain_google_genai",
        "documentation": {}
    },
    {
        "label": "ChatPromptTemplate",
        "importPath": "langchain.prompts",
        "description": "langchain.prompts",
        "isExtraImport": true,
        "detail": "langchain.prompts",
        "documentation": {}
    },
    {
        "label": "ChatPromptTemplate",
        "importPath": "langchain.prompts",
        "description": "langchain.prompts",
        "isExtraImport": true,
        "detail": "langchain.prompts",
        "documentation": {}
    },
    {
        "label": "ChatPromptTemplate",
        "importPath": "langchain.prompts",
        "description": "langchain.prompts",
        "isExtraImport": true,
        "detail": "langchain.prompts",
        "documentation": {}
    },
    {
        "label": "ChatPromptTemplatehi",
        "importPath": "langchain.prompts",
        "description": "langchain.prompts",
        "isExtraImport": true,
        "detail": "langchain.prompts",
        "documentation": {}
    },
    {
        "label": "ChatPromptTemplate",
        "importPath": "langchain.prompts",
        "description": "langchain.prompts",
        "isExtraImport": true,
        "detail": "langchain.prompts",
        "documentation": {}
    },
    {
        "label": "RunnablePassthrough",
        "importPath": "langchain.schema.runnable",
        "description": "langchain.schema.runnable",
        "isExtraImport": true,
        "detail": "langchain.schema.runnable",
        "documentation": {}
    },
    {
        "label": "RunnablePassthrough",
        "importPath": "langchain.schema.runnable",
        "description": "langchain.schema.runnable",
        "isExtraImport": true,
        "detail": "langchain.schema.runnable",
        "documentation": {}
    },
    {
        "label": "RunnablePassthrough",
        "importPath": "langchain.schema.runnable",
        "description": "langchain.schema.runnable",
        "isExtraImport": true,
        "detail": "langchain.schema.runnable",
        "documentation": {}
    },
    {
        "label": "RunnablePassthrough",
        "importPath": "langchain.schema.runnable",
        "description": "langchain.schema.runnable",
        "isExtraImport": true,
        "detail": "langchain.schema.runnable",
        "documentation": {}
    },
    {
        "label": "RunnablePassthrough",
        "importPath": "langchain.schema.runnable",
        "description": "langchain.schema.runnable",
        "isExtraImport": true,
        "detail": "langchain.schema.runnable",
        "documentation": {}
    },
    {
        "label": "StrOutputParser",
        "importPath": "langchain.schema.output_parser",
        "description": "langchain.schema.output_parser",
        "isExtraImport": true,
        "detail": "langchain.schema.output_parser",
        "documentation": {}
    },
    {
        "label": "StrOutputParser",
        "importPath": "langchain.schema.output_parser",
        "description": "langchain.schema.output_parser",
        "isExtraImport": true,
        "detail": "langchain.schema.output_parser",
        "documentation": {}
    },
    {
        "label": "StrOutputParser",
        "importPath": "langchain.schema.output_parser",
        "description": "langchain.schema.output_parser",
        "isExtraImport": true,
        "detail": "langchain.schema.output_parser",
        "documentation": {}
    },
    {
        "label": "StrOutputParser",
        "importPath": "langchain.schema.output_parser",
        "description": "langchain.schema.output_parser",
        "isExtraImport": true,
        "detail": "langchain.schema.output_parser",
        "documentation": {}
    },
    {
        "label": "StrOutputParser",
        "importPath": "langchain.schema.output_parser",
        "description": "langchain.schema.output_parser",
        "isExtraImport": true,
        "detail": "langchain.schema.output_parser",
        "documentation": {}
    },
    {
        "label": "RecursiveCharacterTextSplitter",
        "importPath": "langchain_text_splitters",
        "description": "langchain_text_splitters",
        "isExtraImport": true,
        "detail": "langchain_text_splitters",
        "documentation": {}
    },
    {
        "label": "RecursiveCharacterTextSplitter",
        "importPath": "langchain_text_splitters",
        "description": "langchain_text_splitters",
        "isExtraImport": true,
        "detail": "langchain_text_splitters",
        "documentation": {}
    },
    {
        "label": "DirectoryLoader",
        "importPath": "langchain_community.document_loaders",
        "description": "langchain_community.document_loaders",
        "isExtraImport": true,
        "detail": "langchain_community.document_loaders",
        "documentation": {}
    },
    {
        "label": "DirectoryLoader",
        "importPath": "langchain_community.document_loaders",
        "description": "langchain_community.document_loaders",
        "isExtraImport": true,
        "detail": "langchain_community.document_loaders",
        "documentation": {}
    },
    {
        "label": "PyPDFLoader",
        "importPath": "langchain_community.document_loaders",
        "description": "langchain_community.document_loaders",
        "isExtraImport": true,
        "detail": "langchain_community.document_loaders",
        "documentation": {}
    },
    {
        "label": "OllamaEmbeddings",
        "importPath": "langchain_community.embeddings",
        "description": "langchain_community.embeddings",
        "isExtraImport": true,
        "detail": "langchain_community.embeddings",
        "documentation": {}
    },
    {
        "label": "HuggingFaceEmbeddings",
        "importPath": "langchain_community.embeddings",
        "description": "langchain_community.embeddings",
        "isExtraImport": true,
        "detail": "langchain_community.embeddings",
        "documentation": {}
    },
    {
        "label": "HuggingFaceEmbeddings",
        "importPath": "langchain_community.embeddings",
        "description": "langchain_community.embeddings",
        "isExtraImport": true,
        "detail": "langchain_community.embeddings",
        "documentation": {}
    },
    {
        "label": "Chroma",
        "importPath": "langchain_community.vectorstores",
        "description": "langchain_community.vectorstores",
        "isExtraImport": true,
        "detail": "langchain_community.vectorstores",
        "documentation": {}
    },
    {
        "label": "Qdrant",
        "importPath": "langchain_community.vectorstores",
        "description": "langchain_community.vectorstores",
        "isExtraImport": true,
        "detail": "langchain_community.vectorstores",
        "documentation": {}
    },
    {
        "label": "Qdrant",
        "importPath": "langchain_community.vectorstores",
        "description": "langchain_community.vectorstores",
        "isExtraImport": true,
        "detail": "langchain_community.vectorstores",
        "documentation": {}
    },
    {
        "label": "ChatMistralAI",
        "importPath": "langchain_mistralai.chat_models",
        "description": "langchain_mistralai.chat_models",
        "isExtraImport": true,
        "detail": "langchain_mistralai.chat_models",
        "documentation": {}
    },
    {
        "label": "ChatPromptTemplate",
        "importPath": "langchain_core.prompts",
        "description": "langchain_core.prompts",
        "isExtraImport": true,
        "detail": "langchain_core.prompts",
        "documentation": {}
    },
    {
        "label": "Groq",
        "importPath": "groq",
        "description": "groq",
        "isExtraImport": true,
        "detail": "groq",
        "documentation": {}
    },
    {
        "label": "Groq",
        "importPath": "groq",
        "description": "groq",
        "isExtraImport": true,
        "detail": "groq",
        "documentation": {}
    },
    {
        "label": "ChatGroq",
        "importPath": "langchain_groq",
        "description": "langchain_groq",
        "isExtraImport": true,
        "detail": "langchain_groq",
        "documentation": {}
    },
    {
        "label": "ChatGroq",
        "importPath": "langchain_groq",
        "description": "langchain_groq",
        "isExtraImport": true,
        "detail": "langchain_groq",
        "documentation": {}
    },
    {
        "label": "ChatGroq",
        "importPath": "langchain_groq",
        "description": "langchain_groq",
        "isExtraImport": true,
        "detail": "langchain_groq",
        "documentation": {}
    },
    {
        "label": "ContextualCompressionRetriever",
        "importPath": "langchain.retrievers",
        "description": "langchain.retrievers",
        "isExtraImport": true,
        "detail": "langchain.retrievers",
        "documentation": {}
    },
    {
        "label": "LLMChainExtractor",
        "importPath": "langchain.retrievers.document_compressors",
        "description": "langchain.retrievers.document_compressors",
        "isExtraImport": true,
        "detail": "langchain.retrievers.document_compressors",
        "documentation": {}
    },
    {
        "label": "QdrantClient",
        "importPath": "qdrant_client",
        "description": "qdrant_client",
        "isExtraImport": true,
        "detail": "qdrant_client",
        "documentation": {}
    },
    {
        "label": "models",
        "importPath": "qdrant_client",
        "description": "qdrant_client",
        "isExtraImport": true,
        "detail": "qdrant_client",
        "documentation": {}
    },
    {
        "label": "QdrantClient",
        "importPath": "qdrant_client",
        "description": "qdrant_client",
        "isExtraImport": true,
        "detail": "qdrant_client",
        "documentation": {}
    },
    {
        "label": "models",
        "importPath": "qdrant_client",
        "description": "qdrant_client",
        "isExtraImport": true,
        "detail": "qdrant_client",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "bin_dir",
        "kind": 5,
        "importPath": "venv.Scripts.activate_this",
        "description": "venv.Scripts.activate_this",
        "peekOfCode": "bin_dir = os.path.dirname(abs_file)\nbase = bin_dir[: -len(\"Scripts\") - 1]  # strip away the bin part from the __file__, plus the path separator\n# prepend bin to PATH (this file is inside the bin directory)\nos.environ[\"PATH\"] = os.pathsep.join([bin_dir, *os.environ.get(\"PATH\", \"\").split(os.pathsep)])\nos.environ[\"VIRTUAL_ENV\"] = base  # virtual env is right above bin directory\nos.environ[\"VIRTUAL_ENV_PROMPT\"] = \"\" or os.path.basename(base)  # noqa: SIM222\n# add the virtual environments libraries to the host python import mechanism\nprev_length = len(sys.path)\nfor lib in \"..\\\\Lib\\\\site-packages\".split(os.pathsep):\n    path = os.path.realpath(os.path.join(bin_dir, lib))",
        "detail": "venv.Scripts.activate_this",
        "documentation": {}
    },
    {
        "label": "base",
        "kind": 5,
        "importPath": "venv.Scripts.activate_this",
        "description": "venv.Scripts.activate_this",
        "peekOfCode": "base = bin_dir[: -len(\"Scripts\") - 1]  # strip away the bin part from the __file__, plus the path separator\n# prepend bin to PATH (this file is inside the bin directory)\nos.environ[\"PATH\"] = os.pathsep.join([bin_dir, *os.environ.get(\"PATH\", \"\").split(os.pathsep)])\nos.environ[\"VIRTUAL_ENV\"] = base  # virtual env is right above bin directory\nos.environ[\"VIRTUAL_ENV_PROMPT\"] = \"\" or os.path.basename(base)  # noqa: SIM222\n# add the virtual environments libraries to the host python import mechanism\nprev_length = len(sys.path)\nfor lib in \"..\\\\Lib\\\\site-packages\".split(os.pathsep):\n    path = os.path.realpath(os.path.join(bin_dir, lib))\n    site.addsitedir(path.decode(\"utf-8\") if \"\" else path)",
        "detail": "venv.Scripts.activate_this",
        "documentation": {}
    },
    {
        "label": "os.environ[\"PATH\"]",
        "kind": 5,
        "importPath": "venv.Scripts.activate_this",
        "description": "venv.Scripts.activate_this",
        "peekOfCode": "os.environ[\"PATH\"] = os.pathsep.join([bin_dir, *os.environ.get(\"PATH\", \"\").split(os.pathsep)])\nos.environ[\"VIRTUAL_ENV\"] = base  # virtual env is right above bin directory\nos.environ[\"VIRTUAL_ENV_PROMPT\"] = \"\" or os.path.basename(base)  # noqa: SIM222\n# add the virtual environments libraries to the host python import mechanism\nprev_length = len(sys.path)\nfor lib in \"..\\\\Lib\\\\site-packages\".split(os.pathsep):\n    path = os.path.realpath(os.path.join(bin_dir, lib))\n    site.addsitedir(path.decode(\"utf-8\") if \"\" else path)\nsys.path[:] = sys.path[prev_length:] + sys.path[0:prev_length]\nsys.real_prefix = sys.prefix",
        "detail": "venv.Scripts.activate_this",
        "documentation": {}
    },
    {
        "label": "os.environ[\"VIRTUAL_ENV\"]",
        "kind": 5,
        "importPath": "venv.Scripts.activate_this",
        "description": "venv.Scripts.activate_this",
        "peekOfCode": "os.environ[\"VIRTUAL_ENV\"] = base  # virtual env is right above bin directory\nos.environ[\"VIRTUAL_ENV_PROMPT\"] = \"\" or os.path.basename(base)  # noqa: SIM222\n# add the virtual environments libraries to the host python import mechanism\nprev_length = len(sys.path)\nfor lib in \"..\\\\Lib\\\\site-packages\".split(os.pathsep):\n    path = os.path.realpath(os.path.join(bin_dir, lib))\n    site.addsitedir(path.decode(\"utf-8\") if \"\" else path)\nsys.path[:] = sys.path[prev_length:] + sys.path[0:prev_length]\nsys.real_prefix = sys.prefix\nsys.prefix = base",
        "detail": "venv.Scripts.activate_this",
        "documentation": {}
    },
    {
        "label": "os.environ[\"VIRTUAL_ENV_PROMPT\"]",
        "kind": 5,
        "importPath": "venv.Scripts.activate_this",
        "description": "venv.Scripts.activate_this",
        "peekOfCode": "os.environ[\"VIRTUAL_ENV_PROMPT\"] = \"\" or os.path.basename(base)  # noqa: SIM222\n# add the virtual environments libraries to the host python import mechanism\nprev_length = len(sys.path)\nfor lib in \"..\\\\Lib\\\\site-packages\".split(os.pathsep):\n    path = os.path.realpath(os.path.join(bin_dir, lib))\n    site.addsitedir(path.decode(\"utf-8\") if \"\" else path)\nsys.path[:] = sys.path[prev_length:] + sys.path[0:prev_length]\nsys.real_prefix = sys.prefix\nsys.prefix = base",
        "detail": "venv.Scripts.activate_this",
        "documentation": {}
    },
    {
        "label": "prev_length",
        "kind": 5,
        "importPath": "venv.Scripts.activate_this",
        "description": "venv.Scripts.activate_this",
        "peekOfCode": "prev_length = len(sys.path)\nfor lib in \"..\\\\Lib\\\\site-packages\".split(os.pathsep):\n    path = os.path.realpath(os.path.join(bin_dir, lib))\n    site.addsitedir(path.decode(\"utf-8\") if \"\" else path)\nsys.path[:] = sys.path[prev_length:] + sys.path[0:prev_length]\nsys.real_prefix = sys.prefix\nsys.prefix = base",
        "detail": "venv.Scripts.activate_this",
        "documentation": {}
    },
    {
        "label": "sys.path[:]",
        "kind": 5,
        "importPath": "venv.Scripts.activate_this",
        "description": "venv.Scripts.activate_this",
        "peekOfCode": "sys.path[:] = sys.path[prev_length:] + sys.path[0:prev_length]\nsys.real_prefix = sys.prefix\nsys.prefix = base",
        "detail": "venv.Scripts.activate_this",
        "documentation": {}
    },
    {
        "label": "sys.real_prefix",
        "kind": 5,
        "importPath": "venv.Scripts.activate_this",
        "description": "venv.Scripts.activate_this",
        "peekOfCode": "sys.real_prefix = sys.prefix\nsys.prefix = base",
        "detail": "venv.Scripts.activate_this",
        "documentation": {}
    },
    {
        "label": "sys.prefix",
        "kind": 5,
        "importPath": "venv.Scripts.activate_this",
        "description": "venv.Scripts.activate_this",
        "peekOfCode": "sys.prefix = base",
        "detail": "venv.Scripts.activate_this",
        "documentation": {}
    },
    {
        "label": "ask_question",
        "kind": 2,
        "importPath": "Gemini",
        "description": "Gemini",
        "peekOfCode": "def ask_question(question):\n    print(\"Answer:\\t\", end=\" \", flush=True)\n    for chunk in rag_chain.stream(question):\n        print(chunk, end=\"\", flush=True)\n    print(\"\\n\")\n# Example usage\nif __name__ == \"__main__\":\n    while True:\n        user_question = input(\"Ask a question (or type 'quit' to exit): \")\n        if user_question.lower() == 'quit':",
        "detail": "Gemini",
        "documentation": {}
    },
    {
        "label": "embeddings",
        "kind": 5,
        "importPath": "Gemini",
        "description": "Gemini",
        "peekOfCode": "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\", show_progress=False)\n# embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\ndb = Chroma(persist_directory=\"./db-mawared\",\n            embedding_function=embeddings)\n# Create retriever\nretriever = db.as_retriever(\n    search_type=\"similarity\",\n    search_kwargs= {\"k\": 5}\n)\nload_dotenv()",
        "detail": "Gemini",
        "documentation": {}
    },
    {
        "label": "db",
        "kind": 5,
        "importPath": "Gemini",
        "description": "Gemini",
        "peekOfCode": "db = Chroma(persist_directory=\"./db-mawared\",\n            embedding_function=embeddings)\n# Create retriever\nretriever = db.as_retriever(\n    search_type=\"similarity\",\n    search_kwargs= {\"k\": 5}\n)\nload_dotenv()\ngenai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\nllm = ChatGoogleGenerativeAI(",
        "detail": "Gemini",
        "documentation": {}
    },
    {
        "label": "retriever",
        "kind": 5,
        "importPath": "Gemini",
        "description": "Gemini",
        "peekOfCode": "retriever = db.as_retriever(\n    search_type=\"similarity\",\n    search_kwargs= {\"k\": 5}\n)\nload_dotenv()\ngenai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\nllm = ChatGoogleGenerativeAI(\n    model=\"gemini-1.5-Flash\",\n    temperature=0,\n    max_tokens=None,",
        "detail": "Gemini",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "Gemini",
        "description": "Gemini",
        "peekOfCode": "llm = ChatGoogleGenerativeAI(\n    model=\"gemini-1.5-Flash\",\n    temperature=0,\n    max_tokens=None,\n    timeout=None,\n    max_retries=2,\n)\n# Create prompt template\ntemplate = \"\"\"\nYou are an expert assistant specializing in the Mawared HR System. Your role is to answer the user's question based strictly on the provided context. If the context does not contain the answer, you should ask clarifying questions to gather more information.",
        "detail": "Gemini",
        "documentation": {}
    },
    {
        "label": "template",
        "kind": 5,
        "importPath": "Gemini",
        "description": "Gemini",
        "peekOfCode": "template = \"\"\"\nYou are an expert assistant specializing in the Mawared HR System. Your role is to answer the user's question based strictly on the provided context. If the context does not contain the answer, you should ask clarifying questions to gather more information.\nMake sure to:\n1. Use only the provided context to generate the answer.\n2. Be concise and direct.\n3. If the context is insufficient, ask relevant follow-up questions instead of speculating.\n4. Only answer from the context.\nContext:\n{context}\nQuestion: {question}",
        "detail": "Gemini",
        "documentation": {}
    },
    {
        "label": "prompt",
        "kind": 5,
        "importPath": "Gemini",
        "description": "Gemini",
        "peekOfCode": "prompt = ChatPromptTemplate.from_template(template)\n# Create the RAG chain using LCEL with prompt printing and streaming output\nrag_chain = (\n    {\"context\": retriever, \"question\": RunnablePassthrough()}\n    | prompt\n    | llm\n    | StrOutputParser()\n)\n# Function to ask questions\ndef ask_question(question):",
        "detail": "Gemini",
        "documentation": {}
    },
    {
        "label": "rag_chain",
        "kind": 5,
        "importPath": "Gemini",
        "description": "Gemini",
        "peekOfCode": "rag_chain = (\n    {\"context\": retriever, \"question\": RunnablePassthrough()}\n    | prompt\n    | llm\n    | StrOutputParser()\n)\n# Function to ask questions\ndef ask_question(question):\n    print(\"Answer:\\t\", end=\" \", flush=True)\n    for chunk in rag_chain.stream(question):",
        "detail": "Gemini",
        "documentation": {}
    },
    {
        "label": "loader",
        "kind": 5,
        "importPath": "Indexer",
        "description": "Indexer",
        "peekOfCode": "loader = DirectoryLoader(\"Data\", glob=\"**/*.pdf\")\nprint(\"pdf loaded loader\")\ndocuments = loader.load()\nprint(len(documents))\n# # Create embeddingsclear\nembeddings = OllamaEmbeddings(model=\"nomic-embed-text\", show_progress=True)\n# # Create Semantic Text Splitter\n# text_splitter = SemanticChunker(embeddings, breakpoint_threshold_type=\"interquartile\")\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=500,",
        "detail": "Indexer",
        "documentation": {}
    },
    {
        "label": "documents",
        "kind": 5,
        "importPath": "Indexer",
        "description": "Indexer",
        "peekOfCode": "documents = loader.load()\nprint(len(documents))\n# # Create embeddingsclear\nembeddings = OllamaEmbeddings(model=\"nomic-embed-text\", show_progress=True)\n# # Create Semantic Text Splitter\n# text_splitter = SemanticChunker(embeddings, breakpoint_threshold_type=\"interquartile\")\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=500,\n    chunk_overlap=300,\n    add_start_index=True,",
        "detail": "Indexer",
        "documentation": {}
    },
    {
        "label": "embeddings",
        "kind": 5,
        "importPath": "Indexer",
        "description": "Indexer",
        "peekOfCode": "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\", show_progress=True)\n# # Create Semantic Text Splitter\n# text_splitter = SemanticChunker(embeddings, breakpoint_threshold_type=\"interquartile\")\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=500,\n    chunk_overlap=300,\n    add_start_index=True,\n)\n# # Split documents into chunks\ntexts = text_splitter.split_documents(documents)",
        "detail": "Indexer",
        "documentation": {}
    },
    {
        "label": "text_splitter",
        "kind": 5,
        "importPath": "Indexer",
        "description": "Indexer",
        "peekOfCode": "text_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=500,\n    chunk_overlap=300,\n    add_start_index=True,\n)\n# # Split documents into chunks\ntexts = text_splitter.split_documents(documents)\n# # Create vector store\nvectorstore = Chroma.from_documents(\n    documents=texts, ",
        "detail": "Indexer",
        "documentation": {}
    },
    {
        "label": "texts",
        "kind": 5,
        "importPath": "Indexer",
        "description": "Indexer",
        "peekOfCode": "texts = text_splitter.split_documents(documents)\n# # Create vector store\nvectorstore = Chroma.from_documents(\n    documents=texts, \n    embedding= embeddings,\n    persist_directory=\"./db-mawared\")\nprint(\"vectorstore created\")",
        "detail": "Indexer",
        "documentation": {}
    },
    {
        "label": "vectorstore",
        "kind": 5,
        "importPath": "Indexer",
        "description": "Indexer",
        "peekOfCode": "vectorstore = Chroma.from_documents(\n    documents=texts, \n    embedding= embeddings,\n    persist_directory=\"./db-mawared\")\nprint(\"vectorstore created\")",
        "detail": "Indexer",
        "documentation": {}
    },
    {
        "label": "ask_question",
        "kind": 2,
        "importPath": "Mamba",
        "description": "Mamba",
        "peekOfCode": "def ask_question(question):\n    print(\"Answer:\\t\", end=\" \", flush=True)\n    for chunk in rag_chain.stream(question):\n        print(chunk, end=\"\", flush=True)\n    print(\"\\n\")\n# Example usage\nif __name__ == \"__main__\":\n    while True:\n        user_question = input(\"Ask a question (or type 'quit' to exit): \")\n        if user_question.lower() == 'quit':",
        "detail": "Mamba",
        "documentation": {}
    },
    {
        "label": "embeddings",
        "kind": 5,
        "importPath": "Mamba",
        "description": "Mamba",
        "peekOfCode": "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\", show_progress=False)\n# embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\ndb = Chroma(persist_directory=\"./db-mawared\",\n            embedding_function=embeddings)\n# Create retriever\nretriever = db.as_retriever(\n    search_type=\"similarity\",\n    search_kwargs= {\"k\": 3}\n)\nload_dotenv()",
        "detail": "Mamba",
        "documentation": {}
    },
    {
        "label": "db",
        "kind": 5,
        "importPath": "Mamba",
        "description": "Mamba",
        "peekOfCode": "db = Chroma(persist_directory=\"./db-mawared\",\n            embedding_function=embeddings)\n# Create retriever\nretriever = db.as_retriever(\n    search_type=\"similarity\",\n    search_kwargs= {\"k\": 3}\n)\nload_dotenv()\nos.environ[\"MISTRAL_API_KEY\"] = os.getenv(\"MISTRAL_API_KEY\")\nllm = model = ChatMistralAI(model=\"open-codestral-mamba\")",
        "detail": "Mamba",
        "documentation": {}
    },
    {
        "label": "retriever",
        "kind": 5,
        "importPath": "Mamba",
        "description": "Mamba",
        "peekOfCode": "retriever = db.as_retriever(\n    search_type=\"similarity\",\n    search_kwargs= {\"k\": 3}\n)\nload_dotenv()\nos.environ[\"MISTRAL_API_KEY\"] = os.getenv(\"MISTRAL_API_KEY\")\nllm = model = ChatMistralAI(model=\"open-codestral-mamba\")\n# Create prompt template\ntemplate = \"\"\"\nYou are an expert assistant specializing in the Mawared HR System. Your role is to answer user questions based strictly on the provided context. If the context is insufficient, ask clarifying questions to gather more information.",
        "detail": "Mamba",
        "documentation": {}
    },
    {
        "label": "os.environ[\"MISTRAL_API_KEY\"]",
        "kind": 5,
        "importPath": "Mamba",
        "description": "Mamba",
        "peekOfCode": "os.environ[\"MISTRAL_API_KEY\"] = os.getenv(\"MISTRAL_API_KEY\")\nllm = model = ChatMistralAI(model=\"open-codestral-mamba\")\n# Create prompt template\ntemplate = \"\"\"\nYou are an expert assistant specializing in the Mawared HR System. Your role is to answer user questions based strictly on the provided context. If the context is insufficient, ask clarifying questions to gather more information.\nGuidelines:\n1. Use only the provided context to generate answers.\n2. Be concise and direct.\n3. If the context is insufficient, ask relevant follow-up questions instead of speculating.\n4. Present answers in numbered steps when appropriate.",
        "detail": "Mamba",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "Mamba",
        "description": "Mamba",
        "peekOfCode": "llm = model = ChatMistralAI(model=\"open-codestral-mamba\")\n# Create prompt template\ntemplate = \"\"\"\nYou are an expert assistant specializing in the Mawared HR System. Your role is to answer user questions based strictly on the provided context. If the context is insufficient, ask clarifying questions to gather more information.\nGuidelines:\n1. Use only the provided context to generate answers.\n2. Be concise and direct.\n3. If the context is insufficient, ask relevant follow-up questions instead of speculating.\n4. Present answers in numbered steps when appropriate.\nWhen responding to a question, follow these steps:",
        "detail": "Mamba",
        "documentation": {}
    },
    {
        "label": "template",
        "kind": 5,
        "importPath": "Mamba",
        "description": "Mamba",
        "peekOfCode": "template = \"\"\"\nYou are an expert assistant specializing in the Mawared HR System. Your role is to answer user questions based strictly on the provided context. If the context is insufficient, ask clarifying questions to gather more information.\nGuidelines:\n1. Use only the provided context to generate answers.\n2. Be concise and direct.\n3. If the context is insufficient, ask relevant follow-up questions instead of speculating.\n4. Present answers in numbered steps when appropriate.\nWhen responding to a question, follow these steps:\n1. Analyze the Question\n   - Carefully read and comprehend the context and details.",
        "detail": "Mamba",
        "documentation": {}
    },
    {
        "label": "prompt",
        "kind": 5,
        "importPath": "Mamba",
        "description": "Mamba",
        "peekOfCode": "prompt = ChatPromptTemplate.from_template(template)\n# Create the RAG chain using LCEL with prompt printing and streaming output\nrag_chain = (\n    {\"context\": retriever, \"question\": RunnablePassthrough()}\n    | prompt\n    | llm\n    | StrOutputParser()\n)\n# Function to ask questions\ndef ask_question(question):",
        "detail": "Mamba",
        "documentation": {}
    },
    {
        "label": "rag_chain",
        "kind": 5,
        "importPath": "Mamba",
        "description": "Mamba",
        "peekOfCode": "rag_chain = (\n    {\"context\": retriever, \"question\": RunnablePassthrough()}\n    | prompt\n    | llm\n    | StrOutputParser()\n)\n# Function to ask questions\ndef ask_question(question):\n    print(\"Answer:\\t\", end=\" \", flush=True)\n    for chunk in rag_chain.stream(question):",
        "detail": "Mamba",
        "documentation": {}
    },
    {
        "label": "ask_question",
        "kind": 2,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "def ask_question(question):\n    print(\"Answer:\\t\", end=\" \", flush=True)\n    for chunk in rag_chain.stream(question):\n        print(chunk, end=\"\", flush=True)\n    print(\"\\n\")\n# Example usage\nif __name__ == \"__main__\":\n    while True:\n        user_question = input(\"Ask a question (or type 'quit' to exit): \")\n        if user_question.lower() == 'quit':",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "embeddings",
        "kind": 5,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\", show_progress=False)\n# embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\ndb = Chroma(persist_directory=\"./db-mawared\",\n            embedding_function=embeddings)\n# Create retriever\nretriever = db.as_retriever(\n    search_type=\"similarity\",\n    search_kwargs= {\"k\": 5}\n)\nload_dotenv()",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "db",
        "kind": 5,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "db = Chroma(persist_directory=\"./db-mawared\",\n            embedding_function=embeddings)\n# Create retriever\nretriever = db.as_retriever(\n    search_type=\"similarity\",\n    search_kwargs= {\"k\": 5}\n)\nload_dotenv()\nos.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API\")\n# local_llm = 'llama3.1",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "retriever",
        "kind": 5,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "retriever = db.as_retriever(\n    search_type=\"similarity\",\n    search_kwargs= {\"k\": 5}\n)\nload_dotenv()\nos.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API\")\n# local_llm = 'llama3.1\n# llm = ChatOllama(model=local_llm)\nllm = ChatGroq(\n    model=\"llama-3.3-70b-versatile\",",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "os.environ[\"GROQ_API_KEY\"]",
        "kind": 5,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API\")\n# local_llm = 'llama3.1\n# llm = ChatOllama(model=local_llm)\nllm = ChatGroq(\n    model=\"llama-3.3-70b-versatile\",\n    temperature=0.1,\n    max_tokens=None,\n    timeout=None,\n    max_retries=2,\n    # other params...",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "llm = ChatGroq(\n    model=\"llama-3.3-70b-versatile\",\n    temperature=0.1,\n    max_tokens=None,\n    timeout=None,\n    max_retries=2,\n    # other params...\n)\n# Create prompt template\ntemplate = \"\"\"",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "template",
        "kind": 5,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "template = \"\"\"\nYou are an expert assistant specializing in the Mawared HR System. Your task is to answer the user's question strictly based on the provided context. If the context lacks sufficient information, ask focused clarifying questions to gather additional details.\nTo improve your responses, follow these steps:\nChain-of-Thought (COT): Break down complex queries into logical steps. Use tags like [Step 1], [Step 2], etc., to label each part of the reasoning process. This helps structure your thinking and ensure clarity. For example:\n[Step 1] Identify the key details in the context relevant to the question.\n[Step 2] Break down any assumptions or information gaps.\n[Step 3] Combine all pieces to form the final, well-reasoned response.\nReasoning: Demonstrate a clear logical connection between the context and your answer at each step. If information is missing or unclear, indicate the gap using tags like [Missing Information] and ask relevant follow-up questions to fill that gap.\nClarity and Precision: Provide direct, concise answers focused only on the context. Avoid including speculative or unrelated information.\nFollow-up Questions: If the context is insufficient, focus on asking specific, relevant questions. Label them as [Clarifying Question] to indicate they are needed to complete the response. For example:",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "prompt",
        "kind": 5,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "prompt = ChatPromptTemplate.from_template(template)\n# Create the RAG chain using LCEL with prompt printing and streaming output\nrag_chain = (\n    {\"context\": retriever, \"question\": RunnablePassthrough()}\n    | prompt\n    | llm\n    | StrOutputParser()\n)\n# Function to ask questions\ndef ask_question(question):",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "rag_chain",
        "kind": 5,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "rag_chain = (\n    {\"context\": retriever, \"question\": RunnablePassthrough()}\n    | prompt\n    | llm\n    | StrOutputParser()\n)\n# Function to ask questions\ndef ask_question(question):\n    print(\"Answer:\\t\", end=\" \", flush=True)\n    for chunk in rag_chain.stream(question):",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "ask_question",
        "kind": 2,
        "importPath": "improvedrag",
        "description": "improvedrag",
        "peekOfCode": "def ask_question(question: str) -> None:\n    try:\n        print(\"Answer:\\t\", end=\" \", flush=True)\n        for chunk in rag_chain.stream(question):\n            print(chunk, end=\"\", flush=True)\n        print(\"\\n\")\n    except Exception as e:\n        print(f\"\\nError processing question: {str(e)}\")\nif __name__ == \"__main__\":\n    while True:",
        "detail": "improvedrag",
        "documentation": {}
    },
    {
        "label": "os.environ[\"GROQ_API_KEY\"]",
        "kind": 5,
        "importPath": "improvedrag",
        "description": "improvedrag",
        "peekOfCode": "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API\")\n# Initialize embeddings and vector store\nembeddings = OllamaEmbeddings(\n    model=\"nomic-embed-text\",\n    show_progress=False,\n)\ndb = Chroma(\n    persist_directory=\"./db-mawared\",\n    embedding_function=embeddings,\n)",
        "detail": "improvedrag",
        "documentation": {}
    },
    {
        "label": "embeddings",
        "kind": 5,
        "importPath": "improvedrag",
        "description": "improvedrag",
        "peekOfCode": "embeddings = OllamaEmbeddings(\n    model=\"nomic-embed-text\",\n    show_progress=False,\n)\ndb = Chroma(\n    persist_directory=\"./db-mawared\",\n    embedding_function=embeddings,\n)\n# Create retriever with MMR search\nretriever = db.as_retriever(",
        "detail": "improvedrag",
        "documentation": {}
    },
    {
        "label": "db",
        "kind": 5,
        "importPath": "improvedrag",
        "description": "improvedrag",
        "peekOfCode": "db = Chroma(\n    persist_directory=\"./db-mawared\",\n    embedding_function=embeddings,\n)\n# Create retriever with MMR search\nretriever = db.as_retriever(\n    search_type=\"mmr\",\n    search_kwargs={\n        \"k\": 5,\n        \"fetch_k\": 8,",
        "detail": "improvedrag",
        "documentation": {}
    },
    {
        "label": "retriever",
        "kind": 5,
        "importPath": "improvedrag",
        "description": "improvedrag",
        "peekOfCode": "retriever = db.as_retriever(\n    search_type=\"mmr\",\n    search_kwargs={\n        \"k\": 5,\n        \"fetch_k\": 8,\n        \"lambda_mult\": 0.7\n    }\n)\n# Initialize LLM\nllm = ChatGroq(",
        "detail": "improvedrag",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "improvedrag",
        "description": "improvedrag",
        "peekOfCode": "llm = ChatGroq(\n    model=\"llama-3.3-70b-versatile\",\n    temperature=0.1,\n    max_tokens=None,\n    timeout=None,\n    max_retries=5\n)\n# Add contextual compression\ncompressor = LLMChainExtractor.from_llm(llm)\ncompression_retriever = ContextualCompressionRetriever(",
        "detail": "improvedrag",
        "documentation": {}
    },
    {
        "label": "compressor",
        "kind": 5,
        "importPath": "improvedrag",
        "description": "improvedrag",
        "peekOfCode": "compressor = LLMChainExtractor.from_llm(llm)\ncompression_retriever = ContextualCompressionRetriever(\n    base_retriever=retriever,\n    base_compressor=compressor\n)\n# Enhanced RAG-focused prompt template\ntemplate = \"\"\"\nYou are an expert assistant specializing in the Mawared HR System. Your task is to answer the user's question strictly based on the provided context. If the context lacks sufficient information, ask focused clarifying questions to gather additional details.\nTo improve your responses, follow these steps:\nChain-of-Thought (COT): Break down complex queries into logical steps. Use tags like [Step 1], [Step 2], etc., to label each part of the reasoning process. This helps structure your thinking and ensure clarity. For example:",
        "detail": "improvedrag",
        "documentation": {}
    },
    {
        "label": "compression_retriever",
        "kind": 5,
        "importPath": "improvedrag",
        "description": "improvedrag",
        "peekOfCode": "compression_retriever = ContextualCompressionRetriever(\n    base_retriever=retriever,\n    base_compressor=compressor\n)\n# Enhanced RAG-focused prompt template\ntemplate = \"\"\"\nYou are an expert assistant specializing in the Mawared HR System. Your task is to answer the user's question strictly based on the provided context. If the context lacks sufficient information, ask focused clarifying questions to gather additional details.\nTo improve your responses, follow these steps:\nChain-of-Thought (COT): Break down complex queries into logical steps. Use tags like [Step 1], [Step 2], etc., to label each part of the reasoning process. This helps structure your thinking and ensure clarity. For example:\n[Step 1] Identify the key details in the context relevant to the question.",
        "detail": "improvedrag",
        "documentation": {}
    },
    {
        "label": "template",
        "kind": 5,
        "importPath": "improvedrag",
        "description": "improvedrag",
        "peekOfCode": "template = \"\"\"\nYou are an expert assistant specializing in the Mawared HR System. Your task is to answer the user's question strictly based on the provided context. If the context lacks sufficient information, ask focused clarifying questions to gather additional details.\nTo improve your responses, follow these steps:\nChain-of-Thought (COT): Break down complex queries into logical steps. Use tags like [Step 1], [Step 2], etc., to label each part of the reasoning process. This helps structure your thinking and ensure clarity. For example:\n[Step 1] Identify the key details in the context relevant to the question.\n[Step 2] Break down any assumptions or information gaps.\n[Step 3] Combine all pieces to form the final, well-reasoned response.\nReasoning: Demonstrate a clear logical connection between the context and your answer at each step. If information is missing or unclear, indicate the gap using tags like [Missing Information] and ask relevant follow-up questions to fill that gap.\nClarity and Precision: Provide direct, concise answers focused only on the context. Avoid including speculative or unrelated information.\nFollow-up Questions: If the context is insufficient, focus on asking specific, relevant questions. Label them as [Clarifying Question] to indicate they are needed to complete the response. For example:",
        "detail": "improvedrag",
        "documentation": {}
    },
    {
        "label": "prompt",
        "kind": 5,
        "importPath": "improvedrag",
        "description": "improvedrag",
        "peekOfCode": "prompt = ChatPromptTemplatehi.from_template(template)\n# Create the RAG chain\nrag_chain = (\n    {\"context\": compression_retriever, \"question\": RunnablePassthrough()}\n    | prompt\n    | llm\n    | StrOutputParser()\n)\n# Function to ask questions\ndef ask_question(question: str) -> None:",
        "detail": "improvedrag",
        "documentation": {}
    },
    {
        "label": "rag_chain",
        "kind": 5,
        "importPath": "improvedrag",
        "description": "improvedrag",
        "peekOfCode": "rag_chain = (\n    {\"context\": compression_retriever, \"question\": RunnablePassthrough()}\n    | prompt\n    | llm\n    | StrOutputParser()\n)\n# Function to ask questions\ndef ask_question(question: str) -> None:\n    try:\n        print(\"Answer:\\t\", end=\" \", flush=True)",
        "detail": "improvedrag",
        "documentation": {}
    },
    {
        "label": "load_and_chunk_documents",
        "kind": 2,
        "importPath": "newindexer",
        "description": "newindexer",
        "peekOfCode": "def load_and_chunk_documents(data_dir=\"Data\", chunk_size=500, chunk_overlap=300):\n    \"\"\"Loads PDF documents, splits them into chunks, and returns the chunks.\"\"\"\n    print(\"Loading PDF documents...\")\n    # use glob **/*.pdf to get all pdf in data folder and subfolders \n    loader = DirectoryLoader(data_dir, glob=\"**/*.pdf\", loader_cls=PyPDFLoader)\n    documents = loader.load()\n    print(f\"Loaded {len(documents)} documents.\")\n    text_splitter = RecursiveCharacterTextSplitter(\n        chunk_size=chunk_size,\n        chunk_overlap=chunk_overlap,",
        "detail": "newindexer",
        "documentation": {}
    },
    {
        "label": "create_vectorstore",
        "kind": 2,
        "importPath": "newindexer",
        "description": "newindexer",
        "peekOfCode": "def create_vectorstore(texts, model_name=\"BAAI/bge-large-en-v1.5\", persist_dir=\"./qdrant_mawared\", collection_name=\"mawared\"):\n    \"\"\"Creates a Qdrant vector store from text chunks.\"\"\"\n    print(\"Creating vector embeddings...\")\n    embeddings = HuggingFaceEmbeddings(model_name=model_name) # Changed to HuggingFaceEmbeddings\n    print(\"Creating vector store...\")\n    # Connect to Qdrant Client\n    client = QdrantClient(\n        url=os.getenv(\"QDRANT_URL\"), \n        api_key=os.getenv(\"QDRANT_API_KEY\"),\n        prefer_grpc=True",
        "detail": "newindexer",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "newindexer",
        "description": "newindexer",
        "peekOfCode": "def main():\n    texts = load_and_chunk_documents()\n    vectorstore = create_vectorstore(texts)\nif __name__ == \"__main__\":\n    main()\n    print(\"Done!\")",
        "detail": "newindexer",
        "documentation": {}
    },
    {
        "label": "ask_question",
        "kind": 2,
        "importPath": "newmain",
        "description": "newmain",
        "peekOfCode": "def ask_question(question):\n    print(\"Answer:\\t\", end=\" \", flush=True)\n    for chunk in rag_chain.stream(question):\n        print(chunk, end=\"\", flush=True)\n    print(\"\\n\")\n# Example usage\nif __name__ == \"__main__\":\n    while True:\n        user_question = input(\"\\n \\n \\n Ask a question (or type 'quit' to exit): \")\n        if user_question.lower() == 'quit':",
        "detail": "newmain",
        "documentation": {}
    },
    {
        "label": "os.environ[\"GROQ_API_KEY\"]",
        "kind": 5,
        "importPath": "newmain",
        "description": "newmain",
        "peekOfCode": "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API\")\n# HuggingFace Embeddings\nembeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-large-en-v1.5\")\n# Qdrant Client Setup\nclient = QdrantClient(\n    url=os.getenv(\"QDRANT_URL\"),\n    api_key=os.getenv(\"QDRANT_API_KEY\"),\n    prefer_grpc=True\n)\ncollection_name = \"mawared\"",
        "detail": "newmain",
        "documentation": {}
    },
    {
        "label": "embeddings",
        "kind": 5,
        "importPath": "newmain",
        "description": "newmain",
        "peekOfCode": "embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-large-en-v1.5\")\n# Qdrant Client Setup\nclient = QdrantClient(\n    url=os.getenv(\"QDRANT_URL\"),\n    api_key=os.getenv(\"QDRANT_API_KEY\"),\n    prefer_grpc=True\n)\ncollection_name = \"mawared\"\n# Try to create collection, handle if it already exists\ntry:",
        "detail": "newmain",
        "documentation": {}
    },
    {
        "label": "client",
        "kind": 5,
        "importPath": "newmain",
        "description": "newmain",
        "peekOfCode": "client = QdrantClient(\n    url=os.getenv(\"QDRANT_URL\"),\n    api_key=os.getenv(\"QDRANT_API_KEY\"),\n    prefer_grpc=True\n)\ncollection_name = \"mawared\"\n# Try to create collection, handle if it already exists\ntry:\n    client.create_collection(\n        collection_name=collection_name,",
        "detail": "newmain",
        "documentation": {}
    },
    {
        "label": "collection_name",
        "kind": 5,
        "importPath": "newmain",
        "description": "newmain",
        "peekOfCode": "collection_name = \"mawared\"\n# Try to create collection, handle if it already exists\ntry:\n    client.create_collection(\n        collection_name=collection_name,\n        vectors_config=models.VectorParams(\n            size=768,  # GTE-large embedding size\n            distance=models.Distance.COSINE\n        ),\n    )",
        "detail": "newmain",
        "documentation": {}
    },
    {
        "label": "db",
        "kind": 5,
        "importPath": "newmain",
        "description": "newmain",
        "peekOfCode": "db = Qdrant(\n    client=client,\n    collection_name=collection_name,\n    embeddings=embeddings,\n)\n# Create retriever\nretriever = db.as_retriever(\n    search_type=\"similarity\",\n    search_kwargs={\"k\": 5}\n)",
        "detail": "newmain",
        "documentation": {}
    },
    {
        "label": "retriever",
        "kind": 5,
        "importPath": "newmain",
        "description": "newmain",
        "peekOfCode": "retriever = db.as_retriever(\n    search_type=\"similarity\",\n    search_kwargs={\"k\": 5}\n)\nllm = ChatGroq(\n    model=\"llama-3.3-70b-versatile\",\n    temperature=0.1,\n    max_tokens=None,\n    timeout=None,\n    max_retries=2,",
        "detail": "newmain",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "newmain",
        "description": "newmain",
        "peekOfCode": "llm = ChatGroq(\n    model=\"llama-3.3-70b-versatile\",\n    temperature=0.1,\n    max_tokens=None,\n    timeout=None,\n    max_retries=2,\n    # other params...\n)\n# Create prompt template\ntemplate = \"\"\"",
        "detail": "newmain",
        "documentation": {}
    },
    {
        "label": "template",
        "kind": 5,
        "importPath": "newmain",
        "description": "newmain",
        "peekOfCode": "template = \"\"\"\nYou are an expert assistant specializing in the Mawared HR System. Your task is to answer the user's question strictly based on the provided context. If the context lacks sufficient information, ask focused clarifying questions to gather additional details.\nTo improve your responses, follow these steps:\n**Chain-of-Thought (COT):** Break down complex queries into logical steps. Use tags like [Step 1], [Step 2], etc., to label each part of the reasoning process. This helps structure your thinking and ensure clarity. For example:\n[Step 1] Identify the key entities and actions in the user's question.\n[Step 2] Search the provided context for information related to these entities and actions within the Mawared system.\n[Step 3] If direct information is found, formulate a concise answer based solely on the context.\n[Step 4] If information is missing or unclear, identify the specific gaps preventing a direct answer.\n[Step 5] Formulate precise clarifying questions to address these gaps and enable a complete answer based on (potential) additional context.\n**Reasoning:** Demonstrate a clear logical connection between the context and your answer at each step. If information is missing or unclear, indicate the gap using tags like [Missing Information] and ask relevant follow-up questions to fill that gap. For example:",
        "detail": "newmain",
        "documentation": {}
    },
    {
        "label": "prompt",
        "kind": 5,
        "importPath": "newmain",
        "description": "newmain",
        "peekOfCode": "prompt = ChatPromptTemplate.from_template(template)\n# Create the RAG chain using LCEL with prompt printing and streaming output\nrag_chain = (\n    {\"context\": retriever, \"question\": RunnablePassthrough()}\n    | prompt\n    | llm\n    | StrOutputParser()\n)\n# Function to ask questions\ndef ask_question(question):",
        "detail": "newmain",
        "documentation": {}
    },
    {
        "label": "rag_chain",
        "kind": 5,
        "importPath": "newmain",
        "description": "newmain",
        "peekOfCode": "rag_chain = (\n    {\"context\": retriever, \"question\": RunnablePassthrough()}\n    | prompt\n    | llm\n    | StrOutputParser()\n)\n# Function to ask questions\ndef ask_question(question):\n    print(\"Answer:\\t\", end=\" \", flush=True)\n    for chunk in rag_chain.stream(question):",
        "detail": "newmain",
        "documentation": {}
    }
]