[
    {
        "label": "RecursiveCharacterTextSplitter",
        "importPath": "langchain_text_splitters",
        "description": "langchain_text_splitters",
        "isExtraImport": true,
        "detail": "langchain_text_splitters",
        "documentation": {}
    },
    {
        "label": "DirectoryLoader",
        "importPath": "langchain_community.document_loaders",
        "description": "langchain_community.document_loaders",
        "isExtraImport": true,
        "detail": "langchain_community.document_loaders",
        "documentation": {}
    },
    {
        "label": "OllamaEmbeddings",
        "importPath": "langchain_community.embeddings",
        "description": "langchain_community.embeddings",
        "isExtraImport": true,
        "detail": "langchain_community.embeddings",
        "documentation": {}
    },
    {
        "label": "Chroma",
        "importPath": "langchain_community.vectorstores",
        "description": "langchain_community.vectorstores",
        "isExtraImport": true,
        "detail": "langchain_community.vectorstores",
        "documentation": {}
    },
    {
        "label": "OllamaEmbeddings",
        "importPath": "langchain_community.embeddings.ollama",
        "description": "langchain_community.embeddings.ollama",
        "isExtraImport": true,
        "detail": "langchain_community.embeddings.ollama",
        "documentation": {}
    },
    {
        "label": "OllamaEmbeddings",
        "importPath": "langchain_community.embeddings.ollama",
        "description": "langchain_community.embeddings.ollama",
        "isExtraImport": true,
        "detail": "langchain_community.embeddings.ollama",
        "documentation": {}
    },
    {
        "label": "Chroma",
        "importPath": "langchain_community.vectorstores.chroma",
        "description": "langchain_community.vectorstores.chroma",
        "isExtraImport": true,
        "detail": "langchain_community.vectorstores.chroma",
        "documentation": {}
    },
    {
        "label": "Chroma",
        "importPath": "langchain_community.vectorstores.chroma",
        "description": "langchain_community.vectorstores.chroma",
        "isExtraImport": true,
        "detail": "langchain_community.vectorstores.chroma",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "ChatPromptTemplate",
        "importPath": "langchain.prompts",
        "description": "langchain.prompts",
        "isExtraImport": true,
        "detail": "langchain.prompts",
        "documentation": {}
    },
    {
        "label": "ChatPromptTemplate",
        "importPath": "langchain.prompts",
        "description": "langchain.prompts",
        "isExtraImport": true,
        "detail": "langchain.prompts",
        "documentation": {}
    },
    {
        "label": "RunnablePassthrough",
        "importPath": "langchain.schema.runnable",
        "description": "langchain.schema.runnable",
        "isExtraImport": true,
        "detail": "langchain.schema.runnable",
        "documentation": {}
    },
    {
        "label": "RunnablePassthrough",
        "importPath": "langchain.schema.runnable",
        "description": "langchain.schema.runnable",
        "isExtraImport": true,
        "detail": "langchain.schema.runnable",
        "documentation": {}
    },
    {
        "label": "StrOutputParser",
        "importPath": "langchain.schema.output_parser",
        "description": "langchain.schema.output_parser",
        "isExtraImport": true,
        "detail": "langchain.schema.output_parser",
        "documentation": {}
    },
    {
        "label": "StrOutputParser",
        "importPath": "langchain.schema.output_parser",
        "description": "langchain.schema.output_parser",
        "isExtraImport": true,
        "detail": "langchain.schema.output_parser",
        "documentation": {}
    },
    {
        "label": "ChatMistralAI",
        "importPath": "langchain_mistralai.chat_models",
        "description": "langchain_mistralai.chat_models",
        "isExtraImport": true,
        "detail": "langchain_mistralai.chat_models",
        "documentation": {}
    },
    {
        "label": "ChatPromptTemplate",
        "importPath": "langchain_core.prompts",
        "description": "langchain_core.prompts",
        "isExtraImport": true,
        "detail": "langchain_core.prompts",
        "documentation": {}
    },
    {
        "label": "ChatOllama",
        "importPath": "langchain_community.chat_models.ollama",
        "description": "langchain_community.chat_models.ollama",
        "isExtraImport": true,
        "detail": "langchain_community.chat_models.ollama",
        "documentation": {}
    },
    {
        "label": "Groq",
        "importPath": "groq",
        "description": "groq",
        "isExtraImport": true,
        "detail": "groq",
        "documentation": {}
    },
    {
        "label": "ChatGroq",
        "importPath": "langchain_groq",
        "description": "langchain_groq",
        "isExtraImport": true,
        "detail": "langchain_groq",
        "documentation": {}
    },
    {
        "label": "loader",
        "kind": 5,
        "importPath": "Indexer",
        "description": "Indexer",
        "peekOfCode": "loader = DirectoryLoader(\"Data\", glob=\"**/*.pdf\")\nprint(\"pdf loaded loader\")\ndocuments = loader.load()\nprint(len(documents))\n# # Create embeddingsclear\nembeddings = OllamaEmbeddings(model=\"nomic-embed-text\", show_progress=True)\n# # Create Semantic Text Splitter\n# text_splitter = SemanticChunker(embeddings, breakpoint_threshold_type=\"interquartile\")\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=3000,",
        "detail": "Indexer",
        "documentation": {}
    },
    {
        "label": "documents",
        "kind": 5,
        "importPath": "Indexer",
        "description": "Indexer",
        "peekOfCode": "documents = loader.load()\nprint(len(documents))\n# # Create embeddingsclear\nembeddings = OllamaEmbeddings(model=\"nomic-embed-text\", show_progress=True)\n# # Create Semantic Text Splitter\n# text_splitter = SemanticChunker(embeddings, breakpoint_threshold_type=\"interquartile\")\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=3000,\n    chunk_overlap=300,\n    add_start_index=True,",
        "detail": "Indexer",
        "documentation": {}
    },
    {
        "label": "embeddings",
        "kind": 5,
        "importPath": "Indexer",
        "description": "Indexer",
        "peekOfCode": "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\", show_progress=True)\n# # Create Semantic Text Splitter\n# text_splitter = SemanticChunker(embeddings, breakpoint_threshold_type=\"interquartile\")\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=3000,\n    chunk_overlap=300,\n    add_start_index=True,\n)\n# # Split documents into chunks\ntexts = text_splitter.split_documents(documents)",
        "detail": "Indexer",
        "documentation": {}
    },
    {
        "label": "text_splitter",
        "kind": 5,
        "importPath": "Indexer",
        "description": "Indexer",
        "peekOfCode": "text_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=3000,\n    chunk_overlap=300,\n    add_start_index=True,\n)\n# # Split documents into chunks\ntexts = text_splitter.split_documents(documents)\n# # Create vector store\nvectorstore = Chroma.from_documents(\n    documents=texts, ",
        "detail": "Indexer",
        "documentation": {}
    },
    {
        "label": "texts",
        "kind": 5,
        "importPath": "Indexer",
        "description": "Indexer",
        "peekOfCode": "texts = text_splitter.split_documents(documents)\n# # Create vector store\nvectorstore = Chroma.from_documents(\n    documents=texts, \n    embedding= embeddings,\n    persist_directory=\"./db-mawared\")\nprint(\"vectorstore created\")",
        "detail": "Indexer",
        "documentation": {}
    },
    {
        "label": "vectorstore",
        "kind": 5,
        "importPath": "Indexer",
        "description": "Indexer",
        "peekOfCode": "vectorstore = Chroma.from_documents(\n    documents=texts, \n    embedding= embeddings,\n    persist_directory=\"./db-mawared\")\nprint(\"vectorstore created\")",
        "detail": "Indexer",
        "documentation": {}
    },
    {
        "label": "ask_question",
        "kind": 2,
        "importPath": "Mamba",
        "description": "Mamba",
        "peekOfCode": "def ask_question(question):\n    print(\"Answer:\\t\", end=\" \", flush=True)\n    for chunk in rag_chain.stream(question):\n        print(chunk, end=\"\", flush=True)\n    print(\"\\n\")\n# Example usage\nif __name__ == \"__main__\":\n    while True:\n        user_question = input(\"Ask a question (or type 'quit' to exit): \")\n        if user_question.lower() == 'quit':",
        "detail": "Mamba",
        "documentation": {}
    },
    {
        "label": "embeddings",
        "kind": 5,
        "importPath": "Mamba",
        "description": "Mamba",
        "peekOfCode": "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\", show_progress=False)\n# embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\ndb = Chroma(persist_directory=\"./db-mawared\",\n            embedding_function=embeddings)\n# Create retriever\nretriever = db.as_retriever(\n    search_type=\"similarity\",\n    search_kwargs= {\"k\": 5}\n)\nload_dotenv()",
        "detail": "Mamba",
        "documentation": {}
    },
    {
        "label": "db",
        "kind": 5,
        "importPath": "Mamba",
        "description": "Mamba",
        "peekOfCode": "db = Chroma(persist_directory=\"./db-mawared\",\n            embedding_function=embeddings)\n# Create retriever\nretriever = db.as_retriever(\n    search_type=\"similarity\",\n    search_kwargs= {\"k\": 5}\n)\nload_dotenv()\nos.environ[\"MISTRAL_API_KEY\"] = os.getenv(\"MISTRAL_API_KEY\")\nllm = model = ChatMistralAI(model=\"open-codestral-mamba\")",
        "detail": "Mamba",
        "documentation": {}
    },
    {
        "label": "retriever",
        "kind": 5,
        "importPath": "Mamba",
        "description": "Mamba",
        "peekOfCode": "retriever = db.as_retriever(\n    search_type=\"similarity\",\n    search_kwargs= {\"k\": 5}\n)\nload_dotenv()\nos.environ[\"MISTRAL_API_KEY\"] = os.getenv(\"MISTRAL_API_KEY\")\nllm = model = ChatMistralAI(model=\"open-codestral-mamba\")\n# Create prompt template\ntemplate = \"\"\"",
        "detail": "Mamba",
        "documentation": {}
    },
    {
        "label": "os.environ[\"MISTRAL_API_KEY\"]",
        "kind": 5,
        "importPath": "Mamba",
        "description": "Mamba",
        "peekOfCode": "os.environ[\"MISTRAL_API_KEY\"] = os.getenv(\"MISTRAL_API_KEY\")\nllm = model = ChatMistralAI(model=\"open-codestral-mamba\")\n# Create prompt template\ntemplate = \"\"\"\nYou are an expert assistant specializing in the Mawared HR System. Your role is to answer the user's question based strictly on the provided context. If the context does not contain the answer, you should ask clarifying questions to gather more information.\nMake sure to:\n1. Use only the provided context to generate the answer.\n2. Be concise and direct.\n3. If the context is insufficient, ask relevant follow-up questions instead of speculating.\n4. Only answer from the context.",
        "detail": "Mamba",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "Mamba",
        "description": "Mamba",
        "peekOfCode": "llm = model = ChatMistralAI(model=\"open-codestral-mamba\")\n# Create prompt template\ntemplate = \"\"\"\nYou are an expert assistant specializing in the Mawared HR System. Your role is to answer the user's question based strictly on the provided context. If the context does not contain the answer, you should ask clarifying questions to gather more information.\nMake sure to:\n1. Use only the provided context to generate the answer.\n2. Be concise and direct.\n3. If the context is insufficient, ask relevant follow-up questions instead of speculating.\n4. Only answer from the context.\nContext:",
        "detail": "Mamba",
        "documentation": {}
    },
    {
        "label": "template",
        "kind": 5,
        "importPath": "Mamba",
        "description": "Mamba",
        "peekOfCode": "template = \"\"\"\nYou are an expert assistant specializing in the Mawared HR System. Your role is to answer the user's question based strictly on the provided context. If the context does not contain the answer, you should ask clarifying questions to gather more information.\nMake sure to:\n1. Use only the provided context to generate the answer.\n2. Be concise and direct.\n3. If the context is insufficient, ask relevant follow-up questions instead of speculating.\n4. Only answer from the context.\nContext:\n{context}\nQuestion: {question}",
        "detail": "Mamba",
        "documentation": {}
    },
    {
        "label": "prompt",
        "kind": 5,
        "importPath": "Mamba",
        "description": "Mamba",
        "peekOfCode": "prompt = ChatPromptTemplate.from_template(template)\n# Create the RAG chain using LCEL with prompt printing and streaming output\nrag_chain = (\n    {\"context\": retriever, \"question\": RunnablePassthrough()}\n    | prompt\n    | llm\n    | StrOutputParser()\n)\n# Function to ask questions\ndef ask_question(question):",
        "detail": "Mamba",
        "documentation": {}
    },
    {
        "label": "rag_chain",
        "kind": 5,
        "importPath": "Mamba",
        "description": "Mamba",
        "peekOfCode": "rag_chain = (\n    {\"context\": retriever, \"question\": RunnablePassthrough()}\n    | prompt\n    | llm\n    | StrOutputParser()\n)\n# Function to ask questions\ndef ask_question(question):\n    print(\"Answer:\\t\", end=\" \", flush=True)\n    for chunk in rag_chain.stream(question):",
        "detail": "Mamba",
        "documentation": {}
    },
    {
        "label": "ask_question",
        "kind": 2,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "def ask_question(question):\n    print(\"Answer:\\t\", end=\" \", flush=True)\n    for chunk in rag_chain.stream(question):\n        print(chunk, end=\"\", flush=True)\n    print(\"\\n\")\n# Example usage\nif __name__ == \"__main__\":\n    while True:\n        user_question = input(\"Ask a question (or type 'quit' to exit): \")\n        if user_question.lower() == 'quit':",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "embeddings",
        "kind": 5,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\", show_progress=False)\n# embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\ndb = Chroma(persist_directory=\"./db-mawared\",\n            embedding_function=embeddings)\n# Create retriever\nretriever = db.as_retriever(\n    search_type=\"similarity\",\n    search_kwargs= {\"k\": 5}\n)\nload_dotenv()",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "db",
        "kind": 5,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "db = Chroma(persist_directory=\"./db-mawared\",\n            embedding_function=embeddings)\n# Create retriever\nretriever = db.as_retriever(\n    search_type=\"similarity\",\n    search_kwargs= {\"k\": 5}\n)\nload_dotenv()\nos.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API\")\n# local_llm = 'llama3.1'",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "retriever",
        "kind": 5,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "retriever = db.as_retriever(\n    search_type=\"similarity\",\n    search_kwargs= {\"k\": 5}\n)\nload_dotenv()\nos.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API\")\n# local_llm = 'llama3.1'\n# llm = ChatOllama(model=local_llm)\nllm = ChatGroq(\n    model=\"llama-3.1-8b-instant\",",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "os.environ[\"GROQ_API_KEY\"]",
        "kind": 5,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API\")\n# local_llm = 'llama3.1'\n# llm = ChatOllama(model=local_llm)\nllm = ChatGroq(\n    model=\"llama-3.1-8b-instant\",\n    temperature=0,\n    max_tokens=None,\n    timeout=None,\n    max_retries=2,\n    # other params...",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "llm = ChatGroq(\n    model=\"llama-3.1-8b-instant\",\n    temperature=0,\n    max_tokens=None,\n    timeout=None,\n    max_retries=2,\n    # other params...\n)\n# Create prompt template\ntemplate = \"\"\"",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "template",
        "kind": 5,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "template = \"\"\"\nYou are an expert assistant specializing in the Mawared HR System. Your role is to answer the user's question based strictly on the provided context. If the context does not contain the answer, you should ask clarifying questions to gather more information.\nMake sure to:\n1. Use only the provided context to generate the answer.\n2. Be concise and direct.\n3. If the context is insufficient, ask relevant follow-up questions instead of speculating.\n4. Only answer from the context.\nContext:\n{context}\nQuestion: {question}",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "prompt",
        "kind": 5,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "prompt = ChatPromptTemplate.from_template(template)\n# Create the RAG chain using LCEL with prompt printing and streaming output\nrag_chain = (\n    {\"context\": retriever, \"question\": RunnablePassthrough()}\n    | prompt\n    | llm\n    | StrOutputParser()\n)\n# Function to ask questions\ndef ask_question(question):",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "rag_chain",
        "kind": 5,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "rag_chain = (\n    {\"context\": retriever, \"question\": RunnablePassthrough()}\n    | prompt\n    | llm\n    | StrOutputParser()\n)\n# Function to ask questions\ndef ask_question(question):\n    print(\"Answer:\\t\", end=\" \", flush=True)\n    for chunk in rag_chain.stream(question):",
        "detail": "main",
        "documentation": {}
    }
]